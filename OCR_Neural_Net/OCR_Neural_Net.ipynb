{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \t The Chars74K dataset\n",
    "\n",
    "# OCR\n",
    "\n",
    "## Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece\n",
    "\n",
    "## Project: IoT OCR Part5\n",
    "---\n",
    "\n",
    "In this project, we implement a solution about is the [Chars74k](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/). Character recognition is a classic pattern recognition problem.The recognition is for latin letters and digits.\n",
    "> **Note**: 62 classes (0-9, A-Z, a-z)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction \n",
    "<img src=\"download.png\"> \n",
    "\n",
    "\n",
    "- **STEP 1** is a pre-processing step where we prepareoy data to be fed in the NN.\n",
    "\n",
    "- **STEP 2** is the Encoder step that takes the picture and converts it into D-dimensional tensor with L-Features with a CNN.This also cold feature extraction step because it detects patterns to help the detection process.\n",
    "- **STEP 3** is the decoder which accepts the Encoder output as input and returns a probability distribution over all the classes with a  DNN(deep neural network) .\n",
    "\n",
    "## The Data\n",
    "The Chars74k dataset consists of:\n",
    "\n",
    "- Image size: 64x64 or 128X128\n",
    "- 7705 characters obtained from natural images\n",
    "- 3410 hand drawn characters using a tablet PC\n",
    "- 62992 synthesised characters from computer font\n",
    "\n",
    "\n",
    "## Requirements\n",
    "- python3\n",
    "- Tansorflow 1.12 gpu\n",
    "- opencv\n",
    "- At least 8GB RAM\n",
    "- At least 3GB GPU RAM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example!\n",
    "<img src=\"unnamed.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theologis\\Desktop\\CAS Lab\\sem-supervised_chars74k\n"
     ]
    }
   ],
   "source": [
    "proj_directory = '/Users/Theologis/Desktop/CAS Lab/sem-supervised_chars74k/' ##TOCHANGE... cd in the ${../../sem-supervised_chars74k}/ dir\n",
    "%cd $proj_directory\n",
    "real_size = (32,32,1)\n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 \n",
    "  At this step we load every image. We also  randomly shuffle data to train,validate and test sets and then plot a radom image for visualisation  purposes .The output of the NN should be a one hot encoded vector.This vector must have 1 in the right class and 0 the other 62 classes.\n",
    "### Improving performance\n",
    "The dataset is quite small (less than 10K images). Data augmentation along with a  [GAN](https://www.tensorflow.org/tutorials/generative/dcgan) model help as increase our model's accuracy from 70%  to $88 $% .The GAN model also add the 63th class witch gives as the probability that the image is latter in the 62 classes. \n",
    "#### Data augmentation Steps\n",
    "- Random rotations between -10 and 10 degrees.\n",
    "- Random translation between -10 and 10 pixels in any direction.\n",
    "- Random zoom between factors of 1 and 1.3.\n",
    "- Gray scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5430/5430 [00:02<00:00, 2642.51i/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6185/6185 [00:02<00:00, 2890.03i/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11487 data successfully loades with shape :  (32, 32, 1)\n",
      "Saving Test Data in the 'test_data' folder...\n"
     ]
    }
   ],
   "source": [
    "from data_gen import load_data\n",
    "train_x = load_data(_,proj_directory=proj_directory,real_size=real_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last steps (Our model)\n",
    "In semi-supervised learning, our goal is still to train a model that takes $x$ as input and generates $y$ as output. However, not all of our training examples have a label $y$. We need to develop an algorithm that is able to get better at classification by studying both labeled $(x, y)$ pairs and unlabeled $x$ examples.\n",
    "\n",
    "To do this for the Chars74K dataset, we'll turn the GAN discriminator into an 62 class discriminator. It will recognize the 10 different classes of real Chars74K, as well as an 63th class of fake images that come from the generator. The discriminator will get to train on real labeled images, real unlabeled images, and fake images. By drawing on three sources of data instead of just one, it will generalize to the test set much better than a traditional classifier trained on only one source of data.The model was form this [paper](https://arxiv.org/pdf/1606.03498.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Theologis\\Desktop\\CAS Lab\\sem-supervised_chars74k\\model.py:106: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Theologis\\Desktop\\CAS Lab\\sem-supervised_chars74k\\model.py:160: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train_utils import GAN\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = GAN(real_size, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is shown bellow.It has 450 of thousand parameters executing approximately  15 million of multiplying(6.9M) and adding operations(7.1M).The most complexity level is the conv2d_2 with 12 million of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model: \"sequential_12\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)           (None, 15, 15, 64)        640       \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)           (None, 3, 3, 64)          36928     \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batc (None, 3, 3, 64)          256       \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)           (None, 1, 1, 128)         73856     \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batc (None, 1, 1, 128)         512       \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)           (None, 1, 1, 128)         147584    \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batc (None, 1, 1, 128)         512       \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)           (None, 1, 1, 128)         147584    \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)         (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)             (None, 62)                7998      \n",
    "=================================================================\n",
    "Total params: 453,054\n",
    "Trainable params: 452,286\n",
    "Non-trainable params: 768\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5430/5430 [00:02<00:00, 2656.74i/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6185/6185 [00:02<00:00, 2859.50i/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 62/62 [03:08<00:00,  3.03s/i]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 62/62 [01:45<00:00,  1.70s/i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11492 data successfully loades with shape :  (32, 32, 1)\n",
      "Epoch 0\n",
      "\t\tClassifier train accuracy:  0.3358712552294549\n",
      "\t\tClassifier validation accuracy 0.6612215909090909\n",
      "\t\tStep time:  0.024022579193115234\n",
      "\t\tEpoch time:  65.12619996070862\n",
      "Epoch 1\n",
      "\t\tClassifier train accuracy:  0.7079455655825733\n",
      "\t\tClassifier validation accuracy 0.7173295454545454\n",
      "\t\tStep time:  0.02602362632751465\n",
      "\t\tEpoch time:  64.45401573181152\n",
      "Epoch 2\n",
      "\t\tClassifier train accuracy:  0.7844005962780708\n",
      "\t\tClassifier validation accuracy 0.7487571022727273\n",
      "\t\tStep time:  0.025022268295288086\n",
      "\t\tEpoch time:  64.32308387756348\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./savedmodel\\saved_model.pb\n",
      "\n",
      "Classifier test accuracy 0.7890625\n"
     ]
    }
   ],
   "source": [
    "from data_gen import Feature_Extraction\n",
    "#from train_utils import train\n",
    "from train_utils import train\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "##Data instantiation \n",
    "dataset = Feature_Extraction(minibatch_size=batch_size,proj_directory= proj_directory,real_size=real_size )\n",
    "###Train model\n",
    "train_accuracies, valid_accuracies = train(net, dataset, epochs, batch_size, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2e166b65550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(train_accuracies, label='Train', alpha=0.5)\n",
    "plt.plot(valid_accuracies, label='Vlid', alpha=0.5)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savedmodel\\variables\\variables\n",
      "Output node/s name : discriminator/out:0\n",
      "Prediction of the first image is : A\n",
      "P is real is : 0.9975901656834008\n",
      "Classification time:  0.0007691739797592163\n"
     ]
    }
   ],
   "source": [
    "from Prediction import Run_pred\n",
    "image = \"/Users/Theologis/Desktop/CAS Lab/sem-supervised_chars74k/dataset/ICDAR 2003/TrialTest Set/char/43/4211.jpg\"\n",
    "#image = \"/Users/Theologis/Desktop/CAS Lab/sem-supervised_chars74k/images/tempM1.png\n",
    "Run_pred(proj_directory,image,real_size=real_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we load the .pb that created in the training and run the prediction for a given image.The Output node/s name is the input in the next step without the \":0\".In the next step we must specify the output node of the NN. We are going to create the appropriate files for the DNNDK(frozen_model_dnndk.pd).We also  are going to print  all the notes of the model.\"Input_real\" is the input node and \"discriminator/out\" in the bottom is the output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DNNDK inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints\\discriminator.ckpt\n",
      "INFO:tensorflow:Froze 26 variables.\n",
      "INFO:tensorflow:Converted 26 variables to const ops.\n",
      "DNNDK graph successfully created\n",
      "Successfully created the file 'frozen_model_dnndk.pb'.The name of all model's nodes are:\n",
      "prefix/input_real\n",
      "prefix/discriminator/dropout/Identity\n",
      "prefix/discriminator/conv2d/kernel\n",
      "prefix/discriminator/conv2d/kernel/read\n",
      "prefix/discriminator/conv2d/bias\n",
      "prefix/discriminator/conv2d/bias/read\n",
      "prefix/discriminator/conv2d/Conv2D\n",
      "prefix/discriminator/conv2d/BiasAdd\n",
      "prefix/discriminator/conv2d/Relu\n",
      "prefix/discriminator/dropout_1/Identity\n",
      "prefix/discriminator/conv2d_1/kernel\n",
      "prefix/discriminator/conv2d_1/kernel/read\n",
      "prefix/discriminator/conv2d_1/bias\n",
      "prefix/discriminator/conv2d_1/bias/read\n",
      "prefix/discriminator/conv2d_1/Conv2D\n",
      "prefix/discriminator/conv2d_1/BiasAdd\n",
      "prefix/discriminator/conv2d_1/Relu\n",
      "prefix/discriminator/conv2d_2/kernel\n",
      "prefix/discriminator/conv2d_2/kernel/read\n",
      "prefix/discriminator/conv2d_2/bias\n",
      "prefix/discriminator/conv2d_2/bias/read\n",
      "prefix/discriminator/conv2d_2/Conv2D\n",
      "prefix/discriminator/conv2d_2/BiasAdd\n",
      "prefix/discriminator/batch_normalization/gamma\n",
      "prefix/discriminator/batch_normalization/gamma/read\n",
      "prefix/discriminator/batch_normalization/beta\n",
      "prefix/discriminator/batch_normalization/beta/read\n",
      "prefix/discriminator/batch_normalization/moving_mean\n",
      "prefix/discriminator/batch_normalization/moving_mean/read\n",
      "prefix/discriminator/batch_normalization/moving_variance\n",
      "prefix/discriminator/batch_normalization/moving_variance/read\n",
      "prefix/discriminator/batch_normalization/FusedBatchNorm\n",
      "prefix/discriminator/Relu\n",
      "prefix/discriminator/dropout_2/Identity\n",
      "prefix/discriminator/conv2d_3/kernel\n",
      "prefix/discriminator/conv2d_3/kernel/read\n",
      "prefix/discriminator/conv2d_3/bias\n",
      "prefix/discriminator/conv2d_3/bias/read\n",
      "prefix/discriminator/conv2d_3/Conv2D\n",
      "prefix/discriminator/conv2d_3/BiasAdd\n",
      "prefix/discriminator/batch_normalization_1/gamma\n",
      "prefix/discriminator/batch_normalization_1/gamma/read\n",
      "prefix/discriminator/batch_normalization_1/beta\n",
      "prefix/discriminator/batch_normalization_1/beta/read\n",
      "prefix/discriminator/batch_normalization_1/moving_mean\n",
      "prefix/discriminator/batch_normalization_1/moving_mean/read\n",
      "prefix/discriminator/batch_normalization_1/moving_variance\n",
      "prefix/discriminator/batch_normalization_1/moving_variance/read\n",
      "prefix/discriminator/batch_normalization_1/FusedBatchNorm\n",
      "prefix/discriminator/Relu_1\n",
      "prefix/discriminator/conv2d_4/kernel\n",
      "prefix/discriminator/conv2d_4/kernel/read\n",
      "prefix/discriminator/conv2d_4/bias\n",
      "prefix/discriminator/conv2d_4/bias/read\n",
      "prefix/discriminator/conv2d_4/Conv2D\n",
      "prefix/discriminator/conv2d_4/BiasAdd\n",
      "prefix/discriminator/batch_normalization_2/gamma\n",
      "prefix/discriminator/batch_normalization_2/gamma/read\n",
      "prefix/discriminator/batch_normalization_2/beta\n",
      "prefix/discriminator/batch_normalization_2/beta/read\n",
      "prefix/discriminator/batch_normalization_2/moving_mean\n",
      "prefix/discriminator/batch_normalization_2/moving_mean/read\n",
      "prefix/discriminator/batch_normalization_2/moving_variance\n",
      "prefix/discriminator/batch_normalization_2/moving_variance/read\n",
      "prefix/discriminator/batch_normalization_2/FusedBatchNorm\n",
      "prefix/discriminator/Relu_2\n",
      "prefix/discriminator/conv2d_5/kernel\n",
      "prefix/discriminator/conv2d_5/kernel/read\n",
      "prefix/discriminator/conv2d_5/bias\n",
      "prefix/discriminator/conv2d_5/bias/read\n",
      "prefix/discriminator/conv2d_5/Conv2D\n",
      "prefix/discriminator/conv2d_5/BiasAdd\n",
      "prefix/discriminator/conv2d_5/Relu\n",
      "prefix/discriminator/Mean/reduction_indices\n",
      "prefix/discriminator/Mean\n",
      "prefix/discriminator/dense/kernel\n",
      "prefix/discriminator/dense/kernel/read\n",
      "prefix/discriminator/dense/bias\n",
      "prefix/discriminator/dense/bias/read\n",
      "prefix/discriminator/dense/MatMul\n",
      "prefix/discriminator/dense/BiasAdd\n",
      "prefix/discriminator/out\n"
     ]
    }
   ],
   "source": [
    "from Create_DNNDK_files import Create_Frozen_graph\n",
    "Create_Frozen_graph(output_node_names=\"discriminator/out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DECENT_Q\n",
    "We can see below  a script file named **“decen_q.sh”** that can be found in ${dnndk_chars74k}/. Run “sh decent_q.sh” to invoke the DECENT_Q tool to perform quantization with the\n",
    "appropriate parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decent_q quantize \\\n",
    "  --input_frozen_graph frozen_model_dnndk.pb \\\n",
    "  --input_nodes input_real \\\n",
    "  --input_shapes ?,32,32,1 \\\n",
    "  --output_nodes discriminator/out \\\n",
    "  --input_fn data_gen.load_data \\\n",
    "  --method 1 \\\n",
    "  --gpu 0 \\\n",
    "  --calib_iter 10 \\\n",
    "  --output_dir ./quantize_results \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate dnndk model\n",
    "After we run the decen_q.sh script dnndk export two .pd files.The quantize_eval_model.pb under the quantize_results folder is the tensorflow model for evaluation.The foler \".../../sem-supervised_chars74k}/test_data/\" is where the test images were saved in the training prosses.Copy and paste them in the ${DNNDK_chars74k}/OCR_Zedboard direcory.The script files named \"eval.py\" is used to perform evaluation for the float and quantized model respectively.Evaluation is applied to the ICDAR 2003 dataset for the correct ptection that the input is char and the correct classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theologis\\Desktop\\CAS Lab\\dnndk_chars74k\n"
     ]
    }
   ],
   "source": [
    "proj_directory = '/Users/Theologis/Desktop/CAS Lab/dnndk_chars74k/' ##TOCHANGE... cd in the ${../../dnndk_chars74k}/ dir\n",
    "%cd $proj_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11492 data successfully loades with shape :  (32, 32, 1)\n",
      "\n",
      "Classifier test accuracy 0.75438905\n",
      "P is real is accuracy: 0.9224894662921348\n"
     ]
    }
   ],
   "source": [
    "from eval import evaluate_dnndk_model\n",
    "evaluate_dnndk_model(proj_directory = proj_directory,input_node = \"input_real\",output_node = \"discriminator/out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling TensorFlow Model\n",
    "Run The script file, dnnc_Zedboar.sh in $dnndk_chars74k/ to  compile TensorFlow model using DNNC.After the compilation is complite DNNC will generate two files under output_dir folder.Run the model in the DPU as in the user guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-dnndk-1.12",
   "language": "python",
   "name": "tf-dnndk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
